{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIe1GFwBaF6i"
   },
   "source": [
    "**SPARK Big Data**\n",
    "<br>Bu Collab notebookta apache spark ve big data işlemleri yapılacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JITz2XnvZlk6",
    "outputId": "fefec9ec-3679-4937-b6ac-ad9876b0232f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=507bb6431d2e0b638c4569d4919fa31d1851e648917c45481616c0f0669b7e8f\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eTtt1GeeX35",
    "outputId": "a40927cf-33fc-4cb8-98f2-cfebacf36dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "he5CpjkCadYk"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jq905L1UmLhB",
    "outputId": "752eb6ee-e728-4c7a-d5b0-48d87b095a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 16, 3136, 54.760000000000005, 17.64]\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.parallelize([3,4,56,7.4,4.2])\n",
    "sq=rdd.map(lambda x:x*x)\n",
    "print(sq.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uljL4TrrcBu7"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nf_FTeTeWO5"
   },
   "source": [
    "Ratings Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q01Miqmvdrja",
    "outputId": "0bd04953-7cc9-48cd-9b07-356036138724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6111\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21203\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "conf=SparkConf().setMaster('local').setAppName('RatingsHistogram') #\n",
    "sc=SparkContext(conf=conf)\n",
    "lines=sc.textFile('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/ml-100k/u.data')\n",
    "ratings=lines.map(lambda x: x.split()[2])\n",
    "result=ratings.countByValue()\n",
    "sortedresults=collections.OrderedDict(sorted(result.items()))\n",
    "for key,value in sortedresults.items():\n",
    "  print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpZRUIU_f86j"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0TtlfJqn_Kc"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUFJUoZAf-S7"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "lines = sc.textFile('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/1800.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzOESkJToLx9"
   },
   "outputs": [],
   "source": [
    "def parseline(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Pjz0ep3k6Rf"
   },
   "outputs": [],
   "source": [
    "parsedLines = lines.map(parseline)\n",
    "minTemps = parsedLines.filter(lambda x: 'TMIN' in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xp3_xxmbob89",
    "outputId": "56c53ccb-e022-468e-a11b-4ac80a583777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "results = minTemps.collect()\n",
    "for result in results:\n",
    "    print(result[0] + '\\t{:.2f}F'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ_86_3qfoc-"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnDeFXr2o_0L",
    "outputId": "8d5ceaf1-4895-4891-d06b-9cbd74ca7c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t90.14F\n",
      "EZE00100082\t90.14F\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "lines = sc.textFile('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/1800.csv')\n",
    "parsedLines = lines.map(parseline)\n",
    "maxTemps = parsedLines.filter(lambda x: 'TMAX' in x[1])\n",
    "stationTemps = maxTemps.map(lambda x: (x[0], x[2]))\n",
    "maxTemps = stationTemps.reduceByKey(lambda x, y: max(x, y))\n",
    "results = maxTemps.collect()\n",
    "for result in results:\n",
    "    print(result[0] + '\\t{:.2f}F'.format(result[1]))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tiu7-XEAneHo"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "input = sc.textFile('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/book.txt')\n",
    "words=input.flatMap(lambda x: x.split())\n",
    "word_counts=words.countByValue() # kelimelerin frekanslarını bulur\n",
    "for word,count in word_counts.items():\n",
    "  print(word,count)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jt3DsnLwwlL"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsBnx_WzgeJE"
   },
   "source": [
    "https://medium.com/@sengonulserkan/apache-spark-word-count-a024c497fbb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5rVbVZLwBFx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "def normalizeWords(text):\n",
    "  return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "input = sc.textFile('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/book.txt')\n",
    "words=input.flatMap(normalizeWords)\n",
    "\n",
    "word_counts=words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "word_counts_sorted=word_counts.map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "results=word_counts_sorted.collect()\n",
    "\n",
    "for result in results:\n",
    "  count=str(result[0])\n",
    "  word=result[1].encode('ascii', 'ignore')\n",
    "  if (word):\n",
    "    print(word.decode() + \":\\t\\t\" + count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxXYKJDxh3Qz"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V47BLQGhezKC",
    "outputId": "95b6782c-4cb1-4305-f632-ce3184ad6dba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'CAPTAIN AMERICA' is the most popular superhero, with 1933 co-appearances.\n",
      "b'CAPTAIN AMERICA'\n",
      "b'SPIDER-MAN/PETER PAR'\n",
      "b'IRON MAN/TONY STARK '\n",
      "b'THING/BENJAMIN J. GR'\n",
      "b'WOLVERINE/LOGAN '\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def countCoOccurences(line):\n",
    "  fields = line.split()\n",
    "  return (int(fields[0]), len(fields)-1)\n",
    "\n",
    "def parseNames(line):\n",
    "  fields = line.split('\\\"')\n",
    "  return (int(fields[0]), fields[1].encode('utf-8'))\n",
    "\n",
    "\n",
    "names = sc.textFile('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/Marvel-names.txt')\n",
    "namesRdd=names.map(parseNames)\n",
    "lines=sc.textFile('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/Marvel-graph.txt')\n",
    "\n",
    "pairings = lines.map(countCoOccurences)\n",
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x,y: x+y)\n",
    "flipped = totalFriendsByCharacter.map(lambda xy: (xy[1], xy[0]))\n",
    "\n",
    "mostPopular = flipped.max()\n",
    "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
    "print(str(mostPopularName) + \" is the most popular superhero, with \" + str(mostPopular[0]) + \" co-appearances.\")\n",
    "#print top 5 popular name\n",
    "for i in range(5):\n",
    "  print(namesRdd.lookup(flipped.top(5)[i][1])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGlmpGP_kdss"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TdqrE3qoGqR",
    "outputId": "df6d1bcf-ba8e-4b9b-dc31-fd3d2fd92273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8840048840048842\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data=spark.read.csv('/content/drive/MyDrive/Bootcamp_projeler/day 10 Apache spark big data/pima-indians-diabetes.csv',header=True,inferSchema=True)\n",
    "predictors=data.columns[:-1]\n",
    "assembler=VectorAssembler(inputCols=predictors,outputCol='features')\n",
    "data=assembler.transform(data).select('features','Outcome')\n",
    "\n",
    "train_data,test_data=data.randomSplit([0.8,0.2], seed=42)\n",
    "lr=LogisticRegression(featuresCol='features',labelCol='Outcome',maxIter=10)\n",
    "model=lr.fit(train_data)\n",
    "predictions=model.transform(test_data)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator(labelCol='Outcome')\n",
    "evaluator.evaluate(predictions)\n",
    "print('Accuracy:',evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhWN8HN9rwek"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
